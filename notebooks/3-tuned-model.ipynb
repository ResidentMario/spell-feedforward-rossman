{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "                \n",
    "                loss.backward()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20; Batch 0; Loss 55666232.0\n",
      "Epoch 1/20; Batch 100; Loss 57607148.0\n",
      "Epoch 1/20; Batch 200; Loss 60570552.0\n",
      "Epoch 1/20; Batch 300; Loss 58537116.0\n",
      "Epoch 1/20; Batch 400; Loss 58325912.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7bb230c924c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforwardTabularModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4efcd245f3ca>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mlvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/model_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_2.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "                \n",
    "                loss.backward()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)\n",
    "\n",
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)\n",
    "torch.save(model.named_parameters(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
