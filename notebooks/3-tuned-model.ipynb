{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "                \n",
    "                loss.backward()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10; Batch 0; Loss 58466728.0\n",
      "Epoch 1/10; Batch 100; Loss 55778664.0\n",
      "Epoch 1/10; Batch 200; Loss 60028696.0\n",
      "Epoch 1/10; Batch 300; Loss 55061144.0\n",
      "Epoch 1/10; Batch 400; Loss 60435072.0\n",
      "Epoch 1/10; Batch 500; Loss 59426112.0\n",
      "Epoch 1/10; Batch 600; Loss 58592828.0\n",
      "Epoch 1/10; Batch 700; Loss 55222256.0\n",
      "Epoch 1/10; Batch 800; Loss 56130232.0\n",
      "Epoch 1/10; Batch 900; Loss 59648008.0\n",
      "Epoch 1/10; Batch 1000; Loss 55487832.0\n",
      "Epoch 1/10; Batch 1100; Loss 57305680.0\n",
      "Epoch 1/10; Batch 1200; Loss 54781656.0\n",
      "Epoch 1/10; Batch 1300; Loss 53330400.0\n",
      "Epoch 1/10; Batch 1400; Loss 56143504.0\n",
      "Epoch 1/10; Batch 1500; Loss 56687752.0\n",
      "Epoch 1/10; Batch 1600; Loss 55568620.0\n",
      "Epoch 1/10; Average Loss 56310884.0\n",
      "Epoch 2/10; Batch 0; Loss 49425696.0\n",
      "Epoch 2/10; Batch 100; Loss 46253012.0\n",
      "Epoch 2/10; Batch 200; Loss 44589124.0\n",
      "Epoch 2/10; Batch 300; Loss 44965408.0\n",
      "Epoch 2/10; Batch 400; Loss 43334892.0\n",
      "Epoch 2/10; Batch 500; Loss 35975408.0\n",
      "Epoch 2/10; Batch 600; Loss 35261064.0\n",
      "Epoch 2/10; Batch 700; Loss 28800062.0\n",
      "Epoch 2/10; Batch 800; Loss 22451412.0\n",
      "Epoch 2/10; Batch 900; Loss 20852400.0\n",
      "Epoch 2/10; Batch 1000; Loss 19095844.0\n",
      "Epoch 2/10; Batch 1100; Loss 16680376.0\n",
      "Epoch 2/10; Batch 1200; Loss 11301922.0\n",
      "Epoch 2/10; Batch 1300; Loss 9608109.0\n",
      "Epoch 2/10; Batch 1400; Loss 8431483.0\n",
      "Epoch 2/10; Batch 1500; Loss 10004700.0\n",
      "Epoch 2/10; Batch 1600; Loss 8245830.0\n",
      "Epoch 2/10; Average Loss 25614976.0\n",
      "Epoch 3/10; Batch 0; Loss 8828634.0\n",
      "Epoch 3/10; Batch 100; Loss 6053245.5\n",
      "Epoch 3/10; Batch 200; Loss 7478164.5\n",
      "Epoch 3/10; Batch 300; Loss 8280307.0\n",
      "Epoch 3/10; Batch 400; Loss 6885954.0\n",
      "Epoch 3/10; Batch 500; Loss 7362910.0\n",
      "Epoch 3/10; Batch 600; Loss 8811567.0\n",
      "Epoch 3/10; Batch 700; Loss 7330499.5\n",
      "Epoch 3/10; Batch 800; Loss 7070877.0\n",
      "Epoch 3/10; Batch 900; Loss 6759930.0\n",
      "Epoch 3/10; Batch 1000; Loss 8421557.0\n",
      "Epoch 3/10; Batch 1100; Loss 8355164.5\n",
      "Epoch 3/10; Batch 1200; Loss 9129776.0\n",
      "Epoch 3/10; Batch 1300; Loss 8055687.5\n",
      "Epoch 3/10; Batch 1400; Loss 9939975.0\n",
      "Epoch 3/10; Batch 1500; Loss 7947920.0\n",
      "Epoch 3/10; Batch 1600; Loss 6695570.0\n",
      "Epoch 3/10; Average Loss 7811012.5\n",
      "Epoch 4/10; Batch 0; Loss 7056982.0\n",
      "Epoch 4/10; Batch 100; Loss 8240145.5\n",
      "Epoch 4/10; Batch 200; Loss 8368525.5\n",
      "Epoch 4/10; Batch 300; Loss 7778551.5\n",
      "Epoch 4/10; Batch 400; Loss 7692619.5\n",
      "Epoch 4/10; Batch 500; Loss 6899323.5\n",
      "Epoch 4/10; Batch 600; Loss 7118881.0\n",
      "Epoch 4/10; Batch 700; Loss 5988005.5\n",
      "Epoch 4/10; Batch 800; Loss 8133354.0\n",
      "Epoch 4/10; Batch 900; Loss 8465121.0\n",
      "Epoch 4/10; Batch 1000; Loss 6711103.0\n",
      "Epoch 4/10; Batch 1100; Loss 8061192.0\n",
      "Epoch 4/10; Batch 1200; Loss 7503830.0\n",
      "Epoch 4/10; Batch 1300; Loss 7412283.5\n",
      "Epoch 4/10; Batch 1400; Loss 9487708.0\n",
      "Epoch 4/10; Batch 1500; Loss 7002593.0\n",
      "Epoch 4/10; Batch 1600; Loss 7902547.0\n",
      "Epoch 4/10; Average Loss 7640248.5\n",
      "Epoch 5/10; Batch 0; Loss 8465210.0\n",
      "Epoch 5/10; Batch 100; Loss 8896490.0\n",
      "Epoch 5/10; Batch 200; Loss 6925346.0\n",
      "Epoch 5/10; Batch 300; Loss 5904127.0\n",
      "Epoch 5/10; Batch 400; Loss 6066149.0\n",
      "Epoch 5/10; Batch 500; Loss 8014986.0\n",
      "Epoch 5/10; Batch 600; Loss 8810045.0\n",
      "Epoch 5/10; Batch 700; Loss 6286362.5\n",
      "Epoch 5/10; Batch 800; Loss 6371424.0\n",
      "Epoch 5/10; Batch 900; Loss 7815044.5\n",
      "Epoch 5/10; Batch 1000; Loss 8599928.0\n",
      "Epoch 5/10; Batch 1100; Loss 6989426.5\n",
      "Epoch 5/10; Batch 1200; Loss 5784354.0\n",
      "Epoch 5/10; Batch 1300; Loss 6688529.0\n",
      "Epoch 5/10; Batch 1400; Loss 6946279.5\n",
      "Epoch 5/10; Batch 1500; Loss 7163824.0\n",
      "Epoch 5/10; Batch 1600; Loss 6037825.5\n",
      "Epoch 5/10; Average Loss 7513281.5\n",
      "Epoch 6/10; Batch 0; Loss 6915562.0\n",
      "Epoch 6/10; Batch 100; Loss 6253273.0\n",
      "Epoch 6/10; Batch 200; Loss 7606713.0\n",
      "Epoch 6/10; Batch 300; Loss 7192437.0\n",
      "Epoch 6/10; Batch 400; Loss 8288539.0\n",
      "Epoch 6/10; Batch 500; Loss 7975626.5\n",
      "Epoch 6/10; Batch 600; Loss 7694522.0\n",
      "Epoch 6/10; Batch 700; Loss 7631242.0\n",
      "Epoch 6/10; Batch 800; Loss 7095672.0\n",
      "Epoch 6/10; Batch 900; Loss 7298411.0\n",
      "Epoch 6/10; Batch 1000; Loss 6882145.0\n",
      "Epoch 6/10; Batch 1100; Loss 8625277.0\n",
      "Epoch 6/10; Batch 1200; Loss 6335213.0\n",
      "Epoch 6/10; Batch 1300; Loss 7365390.5\n",
      "Epoch 6/10; Batch 1400; Loss 6397811.0\n",
      "Epoch 6/10; Batch 1500; Loss 6206861.0\n",
      "Epoch 6/10; Batch 1600; Loss 7268653.0\n",
      "Epoch 6/10; Average Loss 7389891.0\n",
      "Epoch 7/10; Batch 0; Loss 7576094.0\n",
      "Epoch 7/10; Batch 100; Loss 7843273.5\n",
      "Epoch 7/10; Batch 200; Loss 8117583.0\n",
      "Epoch 7/10; Batch 300; Loss 7399327.0\n",
      "Epoch 7/10; Batch 400; Loss 6215570.0\n",
      "Epoch 7/10; Batch 500; Loss 7304308.0\n",
      "Epoch 7/10; Batch 600; Loss 6903221.0\n",
      "Epoch 7/10; Batch 700; Loss 7548809.5\n",
      "Epoch 7/10; Batch 800; Loss 6385800.5\n",
      "Epoch 7/10; Batch 900; Loss 7645826.0\n",
      "Epoch 7/10; Batch 1000; Loss 8775958.0\n",
      "Epoch 7/10; Batch 1100; Loss 6694846.0\n",
      "Epoch 7/10; Batch 1200; Loss 6964224.0\n",
      "Epoch 7/10; Batch 1300; Loss 7961712.0\n",
      "Epoch 7/10; Batch 1400; Loss 8047713.0\n",
      "Epoch 7/10; Batch 1500; Loss 6847429.0\n",
      "Epoch 7/10; Batch 1600; Loss 7459271.0\n",
      "Epoch 7/10; Average Loss 7294498.0\n",
      "Epoch 8/10; Batch 0; Loss 6759511.0\n",
      "Epoch 8/10; Batch 100; Loss 6341770.0\n",
      "Epoch 8/10; Batch 200; Loss 6680729.0\n",
      "Epoch 8/10; Batch 300; Loss 7481744.5\n",
      "Epoch 8/10; Batch 400; Loss 7705893.0\n",
      "Epoch 8/10; Batch 500; Loss 7027467.0\n",
      "Epoch 8/10; Batch 600; Loss 6973608.5\n",
      "Epoch 8/10; Batch 700; Loss 5541222.0\n",
      "Epoch 8/10; Batch 800; Loss 8624184.0\n",
      "Epoch 8/10; Batch 900; Loss 9353479.0\n",
      "Epoch 8/10; Batch 1000; Loss 7486206.0\n",
      "Epoch 8/10; Batch 1100; Loss 8239498.0\n",
      "Epoch 8/10; Batch 1200; Loss 6433748.0\n",
      "Epoch 8/10; Batch 1300; Loss 7492417.5\n",
      "Epoch 8/10; Batch 1400; Loss 8602208.0\n",
      "Epoch 8/10; Batch 1500; Loss 6915363.0\n",
      "Epoch 8/10; Batch 1600; Loss 7226329.0\n",
      "Epoch 8/10; Average Loss 7215138.0\n",
      "Epoch 9/10; Batch 0; Loss 6806987.0\n",
      "Epoch 9/10; Batch 100; Loss 6547334.0\n",
      "Epoch 9/10; Batch 200; Loss 7476365.0\n",
      "Epoch 9/10; Batch 300; Loss 5981302.5\n",
      "Epoch 9/10; Batch 400; Loss 6560249.0\n",
      "Epoch 9/10; Batch 500; Loss 7692209.0\n",
      "Epoch 9/10; Batch 600; Loss 6596746.5\n",
      "Epoch 9/10; Batch 700; Loss 5888226.0\n",
      "Epoch 9/10; Batch 800; Loss 7021908.5\n",
      "Epoch 9/10; Batch 900; Loss 6769868.5\n",
      "Epoch 9/10; Batch 1000; Loss 5780588.0\n",
      "Epoch 9/10; Batch 1100; Loss 6240360.5\n",
      "Epoch 9/10; Batch 1200; Loss 8715658.0\n",
      "Epoch 9/10; Batch 1300; Loss 7312152.5\n",
      "Epoch 9/10; Batch 1400; Loss 7951333.0\n",
      "Epoch 9/10; Batch 1500; Loss 7506155.5\n",
      "Epoch 9/10; Batch 1600; Loss 6150825.0\n",
      "Epoch 9/10; Average Loss 7157649.5\n",
      "Epoch 10/10; Batch 0; Loss 6892086.0\n",
      "Epoch 10/10; Batch 100; Loss 6802402.0\n",
      "Epoch 10/10; Batch 200; Loss 8837974.0\n",
      "Epoch 10/10; Batch 300; Loss 7550762.0\n",
      "Epoch 10/10; Batch 400; Loss 7568625.5\n",
      "Epoch 10/10; Batch 500; Loss 8304758.0\n",
      "Epoch 10/10; Batch 600; Loss 7251339.0\n",
      "Epoch 10/10; Batch 700; Loss 6737144.0\n",
      "Epoch 10/10; Batch 800; Loss 6494465.0\n",
      "Epoch 10/10; Batch 900; Loss 6201281.0\n",
      "Epoch 10/10; Batch 1000; Loss 7646270.0\n",
      "Epoch 10/10; Batch 1100; Loss 7129104.0\n",
      "Epoch 10/10; Batch 1200; Loss 7325708.0\n",
      "Epoch 10/10; Batch 1300; Loss 7541180.0\n",
      "Epoch 10/10; Batch 1400; Loss 6580847.0\n",
      "Epoch 10/10; Batch 1500; Loss 6569925.0\n",
      "Epoch 10/10; Batch 1600; Loss 7799005.0\n",
      "Epoch 10/10; Average Loss 7140360.5\n"
     ]
    }
   ],
   "source": [
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../models/model_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_2.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "                \n",
    "                loss.backward()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
