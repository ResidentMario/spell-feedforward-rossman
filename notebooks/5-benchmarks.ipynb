{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script writes the versions of the model used for benchmarks (one with mixed training enabled, one without, both with TensorBoard enabled)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /spell/models/model_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /spell/models/model_4.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    'training loss', lv, self.n_epochs * len(batches) + i\n",
    "                )\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)\n",
    "\n",
    "writer = SummaryWriter(f'/spell/tensorboards/model_4')\n",
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)\n",
    "torch.save(model.state_dict(), \"model_4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /spell/models/model_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /spell/models/model_5.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # The support for mixed precision training built into PyTorch 1.6.0\n",
    "                # (still forthcoming; at time-of-writing access to this feature requires\n",
    "                #  checking out the master branch!) comes in the form of two new functions.\n",
    "                #\n",
    "                # torch.cuda.amp.autocast is one of them. This function analyzes the network\n",
    "                # and determines what parts it may run in FP16 instead of FP32. This (along with\n",
    "                # GradScalar) is what enables mixed-precision training.\n",
    "                #\n",
    "                # Requires pytorch >= 1.6.0, e.g. pytorch master at time of writing.\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(X_batch).squeeze()\n",
    "                    loss = self.loss_fn(y_pred, y_batch)\n",
    "                \n",
    "                # loss.backward()\n",
    "                \n",
    "                # scaler is a GradScaler object. This is the other component of the PyTorch mixed\n",
    "                # precision training API. We scale the losses _before_ apply backprop, this helps\n",
    "                # to ensure the model does not diverge due to very small weight updates rounding\n",
    "                # down to zero.\n",
    "                #\n",
    "                # scaler uses an interesting backoff algorithm to determine when it can scale the\n",
    "                # gradient update multiplier up versus when it needs to cut back on that activity.\n",
    "                # when it sees the model update diverge the other way (e.g. a nan or inf value) it\n",
    "                # reduces its multiplier value and discards the offending gradient update.\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    'training loss', lv, self.n_epochs * len(batches) + i\n",
    "                )\n",
    "\n",
    "                # the scalar wraps the optimizer in order to implements its discarding behavior\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # optimizer.step()\n",
    "                \n",
    "                # presumably the scheduler step happens regardless of whether or not the update\n",
    "                # was discarded; interestingly enough this seems to result in a scheduler warning\n",
    "                # UserWarning fakeout\n",
    "                scheduler.step()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)\n",
    "\n",
    "writer = SummaryWriter(f'/spell/tensorboards/model_5')\n",
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)\n",
    "torch.save(model.state_dict(), \"model_5.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
