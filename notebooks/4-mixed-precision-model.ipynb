{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup\n",
    "\n",
    "This workspace needs to be running on a T4, and we need to have a nightly copy of PyTorch 1.6.0 (still forthcoming) installed. Following the instructions on the PyTorch website..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 27 19:13:32 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.4.0\n",
      "Uninstalling torch-1.4.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/convert-caffe2-to-onnx\n",
      "    /usr/local/bin/convert-onnx-to-caffe2\n",
      "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch-1.4.0.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
      "Proceed (y/n)?   Successfully uninstalled torch-1.4.0\n",
      "Found existing installation: torchvision 0.5.0\n",
      "Uninstalling torchvision-0.5.0:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision-0.5.0.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision/*\n",
      "Proceed (y/n)?   Successfully uninstalled torchvision-0.5.0\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following instructions on `https://pytorch.org/` to get the nightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu102/torch-1.6.0.dev20200527-cp37-cp37m-linux_x86_64.whl (812.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 812.8 MB 6.2 kB/s eta 0:00:0112     |███████████████████▏            | 488.0 MB 4.7 MB/s eta 0:01:09     |██████████████████████████▏     | 665.3 MB 11.5 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu102/torchvision-0.7.0.dev20200527-cp37-cp37m-linux_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 645 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.16.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.6.0.dev20200527 torchvision-0.7.0.dev20200527\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                              1.6.0.dev20200527\n",
      "torchvision                        0.7.0.dev20200527\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0.dev20200527'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "\n",
    "        print(\"YO\")\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")                \n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/autograd/anomaly_mode.py:72: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  warnings.warn('Anomaly Detection has been enabled. '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [500, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a7bb230c924c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforwardTabularModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-7816d2d9dea5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;31m# lv = loss.detach().cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    124\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [500, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardTabularModel(\n",
       "  (loss_fn): MSELoss()\n",
       "  (score_fn): MSELoss()\n",
       "  (emb_0): Embedding(1115, 80)\n",
       "  (emb_1): Embedding(7, 4)\n",
       "  (emb_2): Embedding(3, 3)\n",
       "  (emb_3): Embedding(12, 6)\n",
       "  (emb_4): Embedding(31, 10)\n",
       "  (emb_5): Embedding(2, 2)\n",
       "  (emb_6): Embedding(25, 10)\n",
       "  (emb_7): Embedding(26, 10)\n",
       "  (emb_8): Embedding(4, 3)\n",
       "  (emb_9): Embedding(3, 3)\n",
       "  (emb_10): Embedding(4, 3)\n",
       "  (emb_11): Embedding(23, 9)\n",
       "  (emb_12): Embedding(8, 4)\n",
       "  (emb_13): Embedding(12, 6)\n",
       "  (emb_14): Embedding(52, 15)\n",
       "  (emb_15): Embedding(22, 9)\n",
       "  (emb_16): Embedding(6, 4)\n",
       "  (emb_17): Embedding(6, 4)\n",
       "  (emb_18): Embedding(3, 3)\n",
       "  (emb_19): Embedding(3, 3)\n",
       "  (emb_20): Embedding(8, 4)\n",
       "  (emb_21): Embedding(8, 4)\n",
       "  (embedding_dropout): Dropout(p=0.04, inplace=False)\n",
       "  (cont_batch_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (seq_model): Sequential(\n",
       "    (0): Linear(in_features=215, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.001, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.01, inplace=False)\n",
       "    (8): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/model_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_2.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                \n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "                \n",
    "                loss.backward()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)\n",
    "\n",
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)\n",
    "torch.save(model.named_parameters(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
