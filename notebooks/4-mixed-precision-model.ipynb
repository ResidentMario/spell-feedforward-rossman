{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu102/torch-1.6.0.dev20200527-cp37-cp37m-linux_x86_64.whl (812.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 812.8 MB 7.0 kB/s eta 0:00:011    |██▊                             | 68.6 MB 15.3 MB/s eta 0:00:49     |███████████████████▎            | 488.6 MB 4.4 MB/s eta 0:01:14\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu102/torchvision-0.7.0.dev20200527-cp37-cp37m-linux_x86_64.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 36.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.5.0\n",
      "    Uninstalling torchvision-0.5.0:\n",
      "      Successfully uninstalled torchvision-0.5.0\n",
      "Successfully installed torch-1.6.0.dev20200527 torchvision-0.7.0.dev20200527\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /spell/scripts/upgrade_env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /spell/scripts/upgrade_env.sh\n",
    "pip install -U --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
    "pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run instruction:\n",
    "\n",
    "```\n",
    "prodspell run \\\n",
    "  --machine-type t4 \\\n",
    "  --github-url https://github.com/ResidentMario/spell-feedforward-rossman.git \\\n",
    "  --pip kaggle \"chmod +x /spell/scripts/download_data.sh; chmod +x /spell/scripts/upgrade_env.sh; /spell/scripts/download_data.sh; /spell/scripts/upgrade_env.sh; python /spell/models/model_3.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # The support for mixed precision training built into PyTorch 1.6.0\n",
    "                # (still forthcoming; at time-of-writing access to this feature requires\n",
    "                #  checking out the master branch!) comes in the form of two new functions.\n",
    "                #\n",
    "                # torch.cuda.amp.autocast is one of them. This function analyzes the network\n",
    "                # and determines what parts it may run in FP16 instead of FP32. This (along with\n",
    "                # GradScalar) is what enables mixed-precision training.\n",
    "                #\n",
    "                # Requires pytorch >= 1.6.0, e.g. pytorch master at time of writing.\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(X_batch).squeeze()\n",
    "                    loss = self.loss_fn(y_pred, y_batch)\n",
    "                \n",
    "                # loss.backward()\n",
    "                \n",
    "                # scaler is a GradScaler object. This is the other component of the PyTorch mixed\n",
    "                # precision training API. We scale the losses _before_ apply backprop, this helps\n",
    "                # to ensure the model does not diverge due to very small weight updates rounding\n",
    "                # down to zero.\n",
    "                #\n",
    "                # scaler uses an interesting backoff algorithm to determine when it can scale the\n",
    "                # gradient update multiplier up versus when it needs to cut back on that activity.\n",
    "                # when it sees the model update diverge the other way (e.g. a nan or inf value) it\n",
    "                # reduces its multiplier value and discards the offending gradient update.\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "\n",
    "                # the scalar wraps the optimizer in order to implements its discarding behavior\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # optimizer.step()\n",
    "                \n",
    "                # presumable the scheduler step happens regardless of whether or not the update\n",
    "                # was discarded\n",
    "                scheduler.step()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20; Batch 0; Loss 58217960.0\n",
      "Epoch 1/20; Batch 100; Loss 59468224.0\n",
      "Epoch 1/20; Batch 200; Loss 60325976.0\n",
      "Epoch 1/20; Batch 300; Loss 58315440.0\n",
      "Epoch 1/20; Batch 400; Loss 59755976.0\n",
      "Epoch 1/20; Batch 500; Loss 56871004.0\n",
      "Epoch 1/20; Batch 600; Loss 54980084.0\n",
      "Epoch 1/20; Batch 700; Loss 57700352.0\n",
      "Epoch 1/20; Batch 800; Loss 58390996.0\n",
      "Epoch 1/20; Batch 900; Loss 54263216.0\n",
      "Epoch 1/20; Batch 1000; Loss 53340296.0\n",
      "Epoch 1/20; Batch 1100; Loss 57559816.0\n",
      "Epoch 1/20; Batch 1200; Loss 53535976.0\n",
      "Epoch 1/20; Batch 1300; Loss 55911400.0\n",
      "Epoch 1/20; Batch 1400; Loss 52406528.0\n",
      "Epoch 1/20; Batch 1500; Loss 58407784.0\n",
      "Epoch 1/20; Batch 1600; Loss 56865980.0\n",
      "Epoch 1/20; Average Loss 57406140.0\n",
      "Epoch 2/20; Batch 0; Loss 59180240.0\n",
      "Epoch 2/20; Batch 100; Loss 55453708.0\n",
      "Epoch 2/20; Batch 200; Loss 54911808.0\n",
      "Epoch 2/20; Batch 300; Loss 48295280.0\n",
      "Epoch 2/20; Batch 400; Loss 55015188.0\n",
      "Epoch 2/20; Batch 500; Loss 52795512.0\n",
      "Epoch 2/20; Batch 600; Loss 48699496.0\n",
      "Epoch 2/20; Batch 700; Loss 48305036.0\n",
      "Epoch 2/20; Batch 800; Loss 50266288.0\n",
      "Epoch 2/20; Batch 900; Loss 51497944.0\n",
      "Epoch 2/20; Batch 1000; Loss 48940200.0\n",
      "Epoch 2/20; Batch 1100; Loss 47255972.0\n",
      "Epoch 2/20; Batch 1200; Loss 45597992.0\n",
      "Epoch 2/20; Batch 1300; Loss 47793512.0\n",
      "Epoch 2/20; Batch 1400; Loss 43825528.0\n",
      "Epoch 2/20; Batch 1500; Loss 38370804.0\n",
      "Epoch 2/20; Batch 1600; Loss 34912244.0\n",
      "Epoch 2/20; Average Loss 49027664.0\n",
      "Epoch 3/20; Batch 0; Loss 36654384.0\n",
      "Epoch 3/20; Batch 100; Loss 36121136.0\n",
      "Epoch 3/20; Batch 200; Loss 35197632.0\n",
      "Epoch 3/20; Batch 300; Loss 31196196.0\n",
      "Epoch 3/20; Batch 400; Loss 27287516.0\n",
      "Epoch 3/20; Batch 500; Loss 26392560.0\n",
      "Epoch 3/20; Batch 600; Loss 24776454.0\n",
      "Epoch 3/20; Batch 700; Loss 22783656.0\n",
      "Epoch 3/20; Batch 800; Loss 20887116.0\n",
      "Epoch 3/20; Batch 900; Loss 19836778.0\n",
      "Epoch 3/20; Batch 1000; Loss 13162406.0\n",
      "Epoch 3/20; Batch 1100; Loss 13658366.0\n",
      "Epoch 3/20; Batch 1200; Loss 11241276.0\n",
      "Epoch 3/20; Batch 1300; Loss 12350026.0\n",
      "Epoch 3/20; Batch 1400; Loss 9710384.0\n",
      "Epoch 3/20; Batch 1500; Loss 11788118.0\n",
      "Epoch 3/20; Batch 1600; Loss 7896994.5\n",
      "Epoch 3/20; Average Loss 19532344.0\n",
      "Epoch 4/20; Batch 0; Loss 10336179.0\n",
      "Epoch 4/20; Batch 100; Loss 7235477.0\n",
      "Epoch 4/20; Batch 200; Loss 7259581.0\n",
      "Epoch 4/20; Batch 300; Loss 6951378.0\n",
      "Epoch 4/20; Batch 400; Loss 7028382.0\n",
      "Epoch 4/20; Batch 500; Loss 9752764.0\n",
      "Epoch 4/20; Batch 600; Loss 9718574.0\n",
      "Epoch 4/20; Batch 700; Loss 6351837.0\n",
      "Epoch 4/20; Batch 800; Loss 6755921.0\n",
      "Epoch 4/20; Batch 900; Loss 6512298.0\n",
      "Epoch 4/20; Batch 1000; Loss 7346524.0\n",
      "Epoch 4/20; Batch 1100; Loss 8310415.5\n",
      "Epoch 4/20; Batch 1200; Loss 7795184.5\n",
      "Epoch 4/20; Batch 1300; Loss 8158873.0\n",
      "Epoch 4/20; Batch 1400; Loss 7337298.5\n",
      "Epoch 4/20; Batch 1500; Loss 7507340.5\n",
      "Epoch 4/20; Batch 1600; Loss 8698168.0\n",
      "Epoch 4/20; Average Loss 7820867.5\n",
      "Epoch 5/20; Batch 0; Loss 7395501.0\n",
      "Epoch 5/20; Batch 100; Loss 7509127.5\n",
      "Epoch 5/20; Batch 200; Loss 8334635.0\n",
      "Epoch 5/20; Batch 300; Loss 6918970.0\n",
      "Epoch 5/20; Batch 400; Loss 8072570.5\n",
      "Epoch 5/20; Batch 500; Loss 8002339.5\n",
      "Epoch 5/20; Batch 600; Loss 6776016.5\n",
      "Epoch 5/20; Batch 700; Loss 7553856.0\n",
      "Epoch 5/20; Batch 800; Loss 7528946.0\n",
      "Epoch 5/20; Batch 900; Loss 9387450.0\n",
      "Epoch 5/20; Batch 1000; Loss 7052393.0\n",
      "Epoch 5/20; Batch 1100; Loss 7526967.5\n",
      "Epoch 5/20; Batch 1200; Loss 7211529.5\n",
      "Epoch 5/20; Batch 1300; Loss 6958190.0\n",
      "Epoch 5/20; Batch 1400; Loss 5812484.0\n",
      "Epoch 5/20; Batch 1500; Loss 7465242.5\n",
      "Epoch 5/20; Batch 1600; Loss 8879826.0\n",
      "Epoch 5/20; Average Loss 7647812.5\n",
      "Epoch 6/20; Batch 0; Loss 8111227.5\n",
      "Epoch 6/20; Batch 100; Loss 7366699.0\n",
      "Epoch 6/20; Batch 200; Loss 8569562.0\n",
      "Epoch 6/20; Batch 300; Loss 6850369.0\n",
      "Epoch 6/20; Batch 400; Loss 7430696.5\n",
      "Epoch 6/20; Batch 500; Loss 7587618.0\n",
      "Epoch 6/20; Batch 600; Loss 7389105.0\n",
      "Epoch 6/20; Batch 700; Loss 6966089.0\n",
      "Epoch 6/20; Batch 800; Loss 8408748.0\n",
      "Epoch 6/20; Batch 900; Loss 6302135.0\n",
      "Epoch 6/20; Batch 1000; Loss 6198925.5\n",
      "Epoch 6/20; Batch 1100; Loss 6910452.0\n",
      "Epoch 6/20; Batch 1200; Loss 8191523.0\n",
      "Epoch 6/20; Batch 1300; Loss 7282399.5\n",
      "Epoch 6/20; Batch 1400; Loss 9461780.0\n",
      "Epoch 6/20; Batch 1500; Loss 8494352.0\n",
      "Epoch 6/20; Batch 1600; Loss 7994510.0\n",
      "Epoch 6/20; Average Loss 7536708.5\n",
      "Epoch 7/20; Batch 0; Loss 6780560.5\n",
      "Epoch 7/20; Batch 100; Loss 7898513.0\n",
      "Epoch 7/20; Batch 200; Loss 7043332.0\n",
      "Epoch 7/20; Batch 300; Loss 7496265.0\n",
      "Epoch 7/20; Batch 400; Loss 7437779.0\n",
      "Epoch 7/20; Batch 500; Loss 8458982.0\n",
      "Epoch 7/20; Batch 600; Loss 6593680.0\n",
      "Epoch 7/20; Batch 700; Loss 8094753.0\n",
      "Epoch 7/20; Batch 800; Loss 7212887.0\n",
      "Epoch 7/20; Batch 900; Loss 7546135.0\n",
      "Epoch 7/20; Batch 1000; Loss 6851621.0\n",
      "Epoch 7/20; Batch 1100; Loss 7616438.0\n",
      "Epoch 7/20; Batch 1200; Loss 7542242.0\n",
      "Epoch 7/20; Batch 1300; Loss 6134256.0\n",
      "Epoch 7/20; Batch 1400; Loss 7929696.0\n",
      "Epoch 7/20; Batch 1500; Loss 8922314.0\n",
      "Epoch 7/20; Batch 1600; Loss 9581138.0\n",
      "Epoch 7/20; Average Loss 7435123.0\n",
      "Epoch 8/20; Batch 0; Loss 6763948.0\n",
      "Epoch 8/20; Batch 100; Loss 5423259.0\n",
      "Epoch 8/20; Batch 200; Loss 6490411.0\n",
      "Epoch 8/20; Batch 300; Loss 6119603.0\n",
      "Epoch 8/20; Batch 400; Loss 7724020.0\n",
      "Epoch 8/20; Batch 500; Loss 6404386.0\n",
      "Epoch 8/20; Batch 600; Loss 7381305.5\n",
      "Epoch 8/20; Batch 700; Loss 6568554.0\n",
      "Epoch 8/20; Batch 800; Loss 7786638.5\n",
      "Epoch 8/20; Batch 900; Loss 7445581.0\n",
      "Epoch 8/20; Batch 1000; Loss 6876584.0\n",
      "Epoch 8/20; Batch 1100; Loss 5971369.0\n",
      "Epoch 8/20; Batch 1200; Loss 9302624.0\n",
      "Epoch 8/20; Batch 1300; Loss 8332873.0\n",
      "Epoch 8/20; Batch 1400; Loss 6335472.0\n",
      "Epoch 8/20; Batch 1500; Loss 6718825.0\n",
      "Epoch 8/20; Batch 1600; Loss 8070206.5\n",
      "Epoch 8/20; Average Loss 7353529.0\n",
      "Epoch 9/20; Batch 0; Loss 7249139.5\n",
      "Epoch 9/20; Batch 100; Loss 7869886.0\n",
      "Epoch 9/20; Batch 200; Loss 7715167.0\n",
      "Epoch 9/20; Batch 300; Loss 6885930.0\n",
      "Epoch 9/20; Batch 400; Loss 7740590.0\n",
      "Epoch 9/20; Batch 500; Loss 6256550.5\n",
      "Epoch 9/20; Batch 600; Loss 8297244.0\n",
      "Epoch 9/20; Batch 700; Loss 7060317.0\n",
      "Epoch 9/20; Batch 800; Loss 6506416.0\n",
      "Epoch 9/20; Batch 900; Loss 8554865.0\n",
      "Epoch 9/20; Batch 1000; Loss 6752204.0\n",
      "Epoch 9/20; Batch 1100; Loss 7988340.0\n",
      "Epoch 9/20; Batch 1200; Loss 6729096.0\n",
      "Epoch 9/20; Batch 1300; Loss 8435848.0\n",
      "Epoch 9/20; Batch 1400; Loss 8240152.0\n",
      "Epoch 9/20; Batch 1500; Loss 7444763.0\n",
      "Epoch 9/20; Batch 1600; Loss 7014502.0\n",
      "Epoch 9/20; Average Loss 7275499.5\n",
      "Epoch 10/20; Batch 0; Loss 7537637.0\n",
      "Epoch 10/20; Batch 100; Loss 7047017.5\n",
      "Epoch 10/20; Batch 200; Loss 6001196.0\n",
      "Epoch 10/20; Batch 300; Loss 8179148.5\n",
      "Epoch 10/20; Batch 400; Loss 8297059.0\n",
      "Epoch 10/20; Batch 500; Loss 6346299.0\n",
      "Epoch 10/20; Batch 600; Loss 6855746.0\n",
      "Epoch 10/20; Batch 700; Loss 9120854.0\n",
      "Epoch 10/20; Batch 800; Loss 7298571.5\n",
      "Epoch 10/20; Batch 900; Loss 5935387.0\n",
      "Epoch 10/20; Batch 1000; Loss 7871883.0\n",
      "Epoch 10/20; Batch 1100; Loss 8293095.0\n",
      "Epoch 10/20; Batch 1200; Loss 7038522.5\n",
      "Epoch 10/20; Batch 1300; Loss 6467408.5\n",
      "Epoch 10/20; Batch 1400; Loss 9007228.0\n",
      "Epoch 10/20; Batch 1500; Loss 8770379.0\n",
      "Epoch 10/20; Batch 1600; Loss 9039908.0\n",
      "Epoch 10/20; Average Loss 7217329.0\n",
      "Epoch 11/20; Batch 0; Loss 8397924.0\n",
      "Epoch 11/20; Batch 100; Loss 7440771.0\n",
      "Epoch 11/20; Batch 200; Loss 8028857.0\n",
      "Epoch 11/20; Batch 300; Loss 8085909.0\n",
      "Epoch 11/20; Batch 400; Loss 6512029.0\n",
      "Epoch 11/20; Batch 500; Loss 7995192.0\n",
      "Epoch 11/20; Batch 600; Loss 7215187.5\n",
      "Epoch 11/20; Batch 700; Loss 5421002.0\n",
      "Epoch 11/20; Batch 800; Loss 7302300.0\n",
      "Epoch 11/20; Batch 900; Loss 7258385.0\n",
      "Epoch 11/20; Batch 1000; Loss 7870399.5\n",
      "Epoch 11/20; Batch 1100; Loss 6652616.0\n",
      "Epoch 11/20; Batch 1200; Loss 7504894.0\n",
      "Epoch 11/20; Batch 1300; Loss 7556990.0\n",
      "Epoch 11/20; Batch 1400; Loss 8023298.5\n",
      "Epoch 11/20; Batch 1500; Loss 6446794.5\n",
      "Epoch 11/20; Batch 1600; Loss 8352829.0\n",
      "Epoch 11/20; Average Loss 7155122.5\n",
      "Epoch 12/20; Batch 0; Loss 8951580.0\n",
      "Epoch 12/20; Batch 100; Loss 6846275.5\n",
      "Epoch 12/20; Batch 200; Loss 7243559.0\n",
      "Epoch 12/20; Batch 300; Loss 7331266.0\n",
      "Epoch 12/20; Batch 400; Loss 6510669.5\n",
      "Epoch 12/20; Batch 500; Loss 7775952.5\n",
      "Epoch 12/20; Batch 600; Loss 7599128.0\n",
      "Epoch 12/20; Batch 700; Loss 6811947.0\n",
      "Epoch 12/20; Batch 800; Loss 6280675.0\n",
      "Epoch 12/20; Batch 900; Loss 8381842.5\n",
      "Epoch 12/20; Batch 1000; Loss 6978985.0\n",
      "Epoch 12/20; Batch 1100; Loss 6376391.5\n",
      "Epoch 12/20; Batch 1200; Loss 6657116.0\n",
      "Epoch 12/20; Batch 1300; Loss 7543949.5\n",
      "Epoch 12/20; Batch 1400; Loss 6282298.5\n",
      "Epoch 12/20; Batch 1500; Loss 6715063.0\n",
      "Epoch 12/20; Batch 1600; Loss 6706441.5\n",
      "Epoch 12/20; Average Loss 7097640.0\n",
      "Epoch 13/20; Batch 0; Loss 6537860.5\n",
      "Epoch 13/20; Batch 100; Loss 6523237.0\n",
      "Epoch 13/20; Batch 200; Loss 7468159.0\n",
      "Epoch 13/20; Batch 300; Loss 6075310.5\n",
      "Epoch 13/20; Batch 400; Loss 8168479.0\n",
      "Epoch 13/20; Batch 500; Loss 6252356.0\n",
      "Epoch 13/20; Batch 600; Loss 8645856.0\n",
      "Epoch 13/20; Batch 700; Loss 8047384.0\n",
      "Epoch 13/20; Batch 800; Loss 7074603.0\n",
      "Epoch 13/20; Batch 900; Loss 6041428.0\n",
      "Epoch 13/20; Batch 1000; Loss 6774881.5\n",
      "Epoch 13/20; Batch 1100; Loss 6373089.0\n",
      "Epoch 13/20; Batch 1200; Loss 6439305.0\n",
      "Epoch 13/20; Batch 1300; Loss 7119501.0\n",
      "Epoch 13/20; Batch 1400; Loss 6778422.0\n",
      "Epoch 13/20; Batch 1500; Loss 7164739.0\n",
      "Epoch 13/20; Batch 1600; Loss 6532505.5\n",
      "Epoch 13/20; Average Loss 7055273.5\n",
      "Epoch 14/20; Batch 0; Loss 9002869.0\n",
      "Epoch 14/20; Batch 100; Loss 6879398.0\n",
      "Epoch 14/20; Batch 200; Loss 8599418.0\n",
      "Epoch 14/20; Batch 300; Loss 6602861.0\n",
      "Epoch 14/20; Batch 400; Loss 5504879.0\n",
      "Epoch 14/20; Batch 500; Loss 7428169.5\n",
      "Epoch 14/20; Batch 600; Loss 5783422.0\n",
      "Epoch 14/20; Batch 700; Loss 6402076.5\n",
      "Epoch 14/20; Batch 800; Loss 6457785.5\n",
      "Epoch 14/20; Batch 900; Loss 7504080.0\n",
      "Epoch 14/20; Batch 1000; Loss 7245014.5\n",
      "Epoch 14/20; Batch 1100; Loss 6395108.5\n",
      "Epoch 14/20; Batch 1200; Loss 6725453.0\n",
      "Epoch 14/20; Batch 1300; Loss 6264016.0\n",
      "Epoch 14/20; Batch 1400; Loss 6448326.0\n",
      "Epoch 14/20; Batch 1500; Loss 7899843.0\n",
      "Epoch 14/20; Batch 1600; Loss 7045468.5\n",
      "Epoch 14/20; Average Loss 7007942.0\n",
      "Epoch 15/20; Batch 0; Loss 6875279.0\n",
      "Epoch 15/20; Batch 100; Loss 8470373.0\n",
      "Epoch 15/20; Batch 200; Loss 5700915.5\n",
      "Epoch 15/20; Batch 300; Loss 5976476.0\n",
      "Epoch 15/20; Batch 400; Loss 6187219.5\n",
      "Epoch 15/20; Batch 500; Loss 7795867.0\n",
      "Epoch 15/20; Batch 600; Loss 6685360.5\n",
      "Epoch 15/20; Batch 700; Loss 7992157.5\n",
      "Epoch 15/20; Batch 800; Loss 6236577.0\n",
      "Epoch 15/20; Batch 900; Loss 6922885.0\n",
      "Epoch 15/20; Batch 1000; Loss 8908709.0\n",
      "Epoch 15/20; Batch 1100; Loss 7777796.0\n",
      "Epoch 15/20; Batch 1200; Loss 5734217.5\n",
      "Epoch 15/20; Batch 1300; Loss 6898680.0\n",
      "Epoch 15/20; Batch 1400; Loss 7537560.0\n",
      "Epoch 15/20; Batch 1500; Loss 8566514.0\n",
      "Epoch 15/20; Batch 1600; Loss 7095363.0\n",
      "Epoch 15/20; Average Loss 6963061.0\n",
      "Epoch 16/20; Batch 0; Loss 7533819.0\n",
      "Epoch 16/20; Batch 100; Loss 7236680.0\n",
      "Epoch 16/20; Batch 200; Loss 6427365.0\n",
      "Epoch 16/20; Batch 300; Loss 8648557.0\n",
      "Epoch 16/20; Batch 400; Loss 6798659.0\n",
      "Epoch 16/20; Batch 500; Loss 5907053.5\n",
      "Epoch 16/20; Batch 600; Loss 7280966.5\n",
      "Epoch 16/20; Batch 700; Loss 7804531.0\n",
      "Epoch 16/20; Batch 800; Loss 7046984.0\n",
      "Epoch 16/20; Batch 900; Loss 9201153.0\n",
      "Epoch 16/20; Batch 1000; Loss 6488895.0\n",
      "Epoch 16/20; Batch 1100; Loss 5343474.0\n",
      "Epoch 16/20; Batch 1200; Loss 9173270.0\n",
      "Epoch 16/20; Batch 1300; Loss 8182900.0\n",
      "Epoch 16/20; Batch 1400; Loss 7044554.0\n",
      "Epoch 16/20; Batch 1500; Loss 7451198.0\n",
      "Epoch 16/20; Batch 1600; Loss 7255752.0\n",
      "Epoch 16/20; Average Loss 6928366.0\n",
      "Epoch 17/20; Batch 0; Loss 5522605.0\n",
      "Epoch 17/20; Batch 100; Loss 6381427.5\n",
      "Epoch 17/20; Batch 200; Loss 6925986.0\n",
      "Epoch 17/20; Batch 300; Loss 8923668.0\n",
      "Epoch 17/20; Batch 400; Loss 5779105.0\n",
      "Epoch 17/20; Batch 500; Loss 7149769.5\n",
      "Epoch 17/20; Batch 600; Loss 6607182.5\n",
      "Epoch 17/20; Batch 700; Loss 5218290.5\n",
      "Epoch 17/20; Batch 800; Loss 7717357.0\n",
      "Epoch 17/20; Batch 900; Loss 6614024.5\n",
      "Epoch 17/20; Batch 1000; Loss 6267467.0\n",
      "Epoch 17/20; Batch 1100; Loss 6492004.0\n",
      "Epoch 17/20; Batch 1200; Loss 6737121.0\n",
      "Epoch 17/20; Batch 1300; Loss 6976594.5\n",
      "Epoch 17/20; Batch 1400; Loss 6434371.0\n",
      "Epoch 17/20; Batch 1500; Loss 8548452.0\n",
      "Epoch 17/20; Batch 1600; Loss 7178734.0\n",
      "Epoch 17/20; Average Loss 6900799.5\n",
      "Epoch 18/20; Batch 0; Loss 7967268.0\n",
      "Epoch 18/20; Batch 100; Loss 8389959.0\n",
      "Epoch 18/20; Batch 200; Loss 6890905.5\n",
      "Epoch 18/20; Batch 300; Loss 6515082.0\n",
      "Epoch 18/20; Batch 400; Loss 7349603.5\n",
      "Epoch 18/20; Batch 500; Loss 8203559.5\n",
      "Epoch 18/20; Batch 600; Loss 5117588.0\n",
      "Epoch 18/20; Batch 700; Loss 7115685.0\n",
      "Epoch 18/20; Batch 800; Loss 5941789.0\n",
      "Epoch 18/20; Batch 900; Loss 8244020.0\n",
      "Epoch 18/20; Batch 1000; Loss 6908544.5\n",
      "Epoch 18/20; Batch 1100; Loss 8246557.0\n",
      "Epoch 18/20; Batch 1200; Loss 7561863.0\n",
      "Epoch 18/20; Batch 1300; Loss 6723193.0\n",
      "Epoch 18/20; Batch 1400; Loss 6760080.0\n",
      "Epoch 18/20; Batch 1500; Loss 8313397.5\n",
      "Epoch 18/20; Batch 1600; Loss 6083469.0\n",
      "Epoch 18/20; Average Loss 6879568.5\n",
      "Epoch 19/20; Batch 0; Loss 8088856.5\n",
      "Epoch 19/20; Batch 100; Loss 6252085.0\n",
      "Epoch 19/20; Batch 200; Loss 6959585.0\n",
      "Epoch 19/20; Batch 300; Loss 7146778.5\n",
      "Epoch 19/20; Batch 400; Loss 8526260.0\n",
      "Epoch 19/20; Batch 500; Loss 6532740.0\n",
      "Epoch 19/20; Batch 600; Loss 6262961.0\n",
      "Epoch 19/20; Batch 700; Loss 5789925.5\n",
      "Epoch 19/20; Batch 800; Loss 6768597.5\n",
      "Epoch 19/20; Batch 900; Loss 7803405.5\n",
      "Epoch 19/20; Batch 1000; Loss 6106812.0\n",
      "Epoch 19/20; Batch 1100; Loss 7223735.5\n",
      "Epoch 19/20; Batch 1200; Loss 6721470.0\n",
      "Epoch 19/20; Batch 1300; Loss 8143573.0\n",
      "Epoch 19/20; Batch 1400; Loss 6302199.0\n",
      "Epoch 19/20; Batch 1500; Loss 6888099.0\n",
      "Epoch 19/20; Batch 1600; Loss 7760729.5\n",
      "Epoch 19/20; Average Loss 6868626.0\n",
      "Epoch 20/20; Batch 0; Loss 7217531.5\n",
      "Epoch 20/20; Batch 100; Loss 5533581.0\n",
      "Epoch 20/20; Batch 200; Loss 6520419.0\n",
      "Epoch 20/20; Batch 300; Loss 8239458.0\n",
      "Epoch 20/20; Batch 400; Loss 8386736.5\n",
      "Epoch 20/20; Batch 500; Loss 6215714.0\n",
      "Epoch 20/20; Batch 600; Loss 7165999.0\n",
      "Epoch 20/20; Batch 700; Loss 5789215.0\n",
      "Epoch 20/20; Batch 800; Loss 6111575.5\n",
      "Epoch 20/20; Batch 900; Loss 9150432.0\n",
      "Epoch 20/20; Batch 1000; Loss 5859886.0\n",
      "Epoch 20/20; Batch 1100; Loss 6946364.0\n",
      "Epoch 20/20; Batch 1200; Loss 7065420.0\n",
      "Epoch 20/20; Batch 1300; Loss 6555132.0\n",
      "Epoch 20/20; Batch 1400; Loss 6140036.0\n",
      "Epoch 20/20; Batch 1500; Loss 9288108.0\n",
      "Epoch 20/20; Batch 1600; Loss 5697790.5\n",
      "Epoch 20/20; Average Loss 6861348.5\n"
     ]
    }
   ],
   "source": [
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../models/model_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_3.py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('rossmann')\n",
    "train_df = pd.read_pickle('/mnt/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')\n",
    "test_df = pd.read_pickle('/mnt/rossman-fastai-sample/test_clean')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "cat_vars = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw'\n",
    "]\n",
    "cont_vars = [\n",
    "    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'\n",
    "]\n",
    "target_var= 'Sales'\n",
    "\n",
    "\n",
    "class ColumnFilter:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cat_vars + cont_vars]\n",
    "        \n",
    "\n",
    "class GroupLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.labeller = LabelEncoder()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.encoders = {col: None for col in X.columns if col in cat_vars}\n",
    "        for col in self.encoders:\n",
    "            self.encoders[col] = LabelEncoder().fit(\n",
    "                X[col].fillna(value='N/A').values\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = []\n",
    "        categorical_part = np.hstack([\n",
    "            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]\n",
    "            for col in cat_vars\n",
    "        ])\n",
    "        return pd.DataFrame(categorical_part, columns=cat_vars)\n",
    "\n",
    "\n",
    "class GroupNullImputer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.loc[:, cont_vars].fillna(0)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.cf = ColumnFilter()\n",
    "        self.gne = GroupNullImputer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.gle = GroupLabelEncoder().fit(X, y=None)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = self.cf.transform(X)\n",
    "        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))\n",
    "        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)\n",
    "        return X_out\n",
    "\n",
    "\n",
    "X_train_sample = Preprocessor().fit(train_df).transform(train_df)\n",
    "y_train_sample = train_df[target_var]\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666\n",
    "\n",
    "\n",
    "class FeedforwardTabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 512\n",
    "        self.base_lr, self.max_lr = 0.001, 0.003\n",
    "        self.n_epochs = 20\n",
    "        self.cat_vars_embedding_vector_lengths = [\n",
    "            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),\n",
    "            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),\n",
    "            (3, 3), (8, 4), (8, 4)\n",
    "        ]\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Layer 1: embeddings.\n",
    "        self.embeddings = []\n",
    "        for i, (in_size, out_size) in enumerate(self.cat_vars_embedding_vector_lengths):\n",
    "            emb = nn.Embedding(in_size, out_size)\n",
    "            self.embeddings.append(emb)\n",
    "            setattr(self, f'emb_{i}', emb)\n",
    "\n",
    "        # Layer 1: dropout.\n",
    "        self.embedding_dropout = nn.Dropout(0.04)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        self.seq_model = nn.Sequential(*[\n",
    "            nn.Linear(in_features=215, out_features=1000, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.001),\n",
    "            nn.Linear(in_features=1000, out_features=500, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.01),\n",
    "            nn.Linear(in_features=500, out_features=1, bias=True)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: embeddings.\n",
    "        inp_offset = 0\n",
    "        embedding_subvectors = []\n",
    "        for emb in self.embeddings:\n",
    "            index = torch.tensor(inp_offset, dtype=torch.int64).cuda()\n",
    "            inp = torch.index_select(x, dim=1, index=index).long().cuda()\n",
    "            out = emb(inp)\n",
    "            out = out.view(out.shape[2], out.shape[0], 1).squeeze()\n",
    "            embedding_subvectors.append(out)\n",
    "            inp_offset += 1\n",
    "        out_cat = torch.cat(embedding_subvectors)\n",
    "        out_cat = out_cat.view(out_cat.shape[::-1])\n",
    "        \n",
    "        # Layer 1: dropout.\n",
    "        out_cat = self.embedding_dropout(out_cat)\n",
    "        \n",
    "        # Layer 1: batch normalization (of the continuous variables).\n",
    "        out_cont = self.cont_batch_norm(x[:, inp_offset:])\n",
    "        \n",
    "        out = torch.cat((out_cat, out_cont), dim=1)\n",
    "        \n",
    "        # Layers 2 through 9: sequential feedforward model.\n",
    "        out = self.seq_model(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.train()\n",
    "        \n",
    "        # TODO: set a random seed to invoke determinism.\n",
    "        # Cf. GH#11278\n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # OneCycleLR with Adam.\n",
    "        #\n",
    "        # Implementation notes. OneCyceLR by default cycles both the learning rate /and/ the\n",
    "        # momentum value.\n",
    "        # Cf. https://www.kaggle.com/residentmario/one-cycle-learning-rate-schedulers\n",
    "        #\n",
    "        # Optimizers that don't support momentum must use a scheduler with cycle_momentum=False,\n",
    "        # which disables the momentum-tuning behavior. Adam does not support momentum; it has its\n",
    "        # own similar-ish thing built in.\n",
    "        # Cf. https://www.kaggle.com/residentmario/keras-optimizers\n",
    "        #\n",
    "        # This code requires PyTorch >= 1.2 due to a bug, see GH#19003.\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.max_lr,\n",
    "            cycle_momentum=False,\n",
    "            epochs=self.n_epochs,\n",
    "            steps_per_epoch=int(np.ceil(len(X) / self.batch_size)),\n",
    "        )\n",
    "        batches = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X, y),\n",
    "            batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            lvs = []\n",
    "            for i, (X_batch, y_batch) in enumerate(batches):\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # The support for mixed precision training built into PyTorch 1.6.0\n",
    "                # (still forthcoming; at time-of-writing access to this feature requires\n",
    "                #  checking out the master branch!) comes in the form of two new functions.\n",
    "                #\n",
    "                # torch.cuda.amp.autocast is one of them. This function analyzes the network\n",
    "                # and determines what parts it may run in FP16 instead of FP32. This (along with\n",
    "                # GradScalar) is what enables mixed-precision training.\n",
    "                #\n",
    "                # Requires pytorch >= 1.6.0, e.g. pytorch master at time of writing.\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(X_batch).squeeze()\n",
    "                    loss = self.loss_fn(y_pred, y_batch)\n",
    "                \n",
    "                # loss.backward()\n",
    "                \n",
    "                # scaler is a GradScaler object. This is the other component of the PyTorch mixed\n",
    "                # precision training API. We scale the losses _before_ apply backprop, this helps\n",
    "                # to ensure the model does not diverge due to very small weight updates rounding\n",
    "                # down to zero.\n",
    "                #\n",
    "                # scaler uses an interesting backoff algorithm to determine when it can scale the\n",
    "                # gradient update multiplier up versus when it needs to cut back on that activity.\n",
    "                # when it sees the model update diverge the other way (e.g. a nan or inf value) it\n",
    "                # reduces its multiplier value and discards the offending gradient update.\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                lv = loss.detach().cpu().numpy()\n",
    "                lvs.append(lv)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")\n",
    "\n",
    "                # the scalar wraps the optimizer in order to implements its discarding behavior\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # optimizer.step()\n",
    "                \n",
    "                # presumable the scheduler step happens regardless of whether or not the update\n",
    "                # was discarded\n",
    "                scheduler.step()\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{self.n_epochs}; Average Loss {np.mean(lvs)}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(torch.tensor(X, dtype=torch.float32).cuda())\n",
    "        return y_pred.squeeze()\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "        return self.score_fn(y, y_pred)\n",
    "\n",
    "model = FeedforwardTabularModel()\n",
    "model.cuda()\n",
    "model.fit(X_train_sample.values, y_train_sample.values)\n",
    "torch.save(model.named_parameters(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
